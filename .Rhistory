require("htmltools")
install()
document()
devtools::document()
prin("hello")
print("hello")
devtools::install()
print("hola")
install()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
require("sunflower")
require("tidyverse")
require("htmltools")
df_to_formal_metrics = sunflower::IGC_long_phon %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item",
response_col = "response",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
formal_metrics_computed %>% head(8) %>% knitr::kable()
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(match_col = "adj_strict_match_pos")
formal_metrics_computed
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(match_col = "itemL_adj_strict_match_pos")
formal_metrics_computed
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(item_col = item, response_col = response, match_col = "itemL_adj_strict_match_pos")
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(item_col = "item", response_col = "response", match_col = "itemL_adj_strict_match_pos")
positions_accuracy %>% head(8) %>% knitr::kable()
# Convertir targetL a character para evitar problemas al combinar dataframes
positions_accuracy <- positions_accuracy %>%
mutate(targetL = as.character(targetL))
# Duplicar y modificar el dataframe para crear 'positions_general'
positions_general <- positions_accuracy %>%
mutate(targetL = "General")
# Combinar ambos dataframes
positions <- bind_rows(positions_accuracy, positions_general)
# Especificar manualmente los niveles en el orden deseado
desired_levels <- c("3", "4", "5", "6", "7", "8", "9", "10", "11", "12",
"13", "14", "15", "17", "21", "22", "24", "48", "General")
# Convertir correct_pos a numérico y ordenar targetL como factor según desired_levels
positions <- positions %>%
mutate(correct_pos = as.numeric(correct_pos),
targetL = factor(targetL, levels = desired_levels)) %>%
arrange(correct_pos, targetL)
# Definir un conjunto de linetypes que se pueda repetir
custom_linetypes <- rep(c("solid", "dashed", "dotted", "longdash", "dotdash"),
length.out = nlevels(positions$targetL))
# Calcular la precisión y contar el número de observaciones por grupo
plot_positions <- positions %>%
group_by(Position, targetL) %>%
summarize(acc = mean(correct_pos, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x = as.numeric(Position), y = acc, group = targetL,
fill = targetL, color = targetL, lty = targetL)) +
geom_line(size = 0.70, alpha = 0.6) +
geom_point(aes(size = n), shape = 21, color = "black", alpha = 0.6) +
scale_linetype_manual(values = custom_linetypes) +
theme(panel.border = element_rect(colour = "black", fill = NA),
panel.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line = element_blank()) +
ylab("Proportion (%) of correct phonemes") +
xlab("Phoneme position") +
guides(fill = guide_legend(title = "Word Length"),
lty = guide_legend(title = "Word Length"),
color = guide_legend(title = "Word Length"),
size = guide_legend(title = "Datapoints")) +
theme_gray() +
theme(legend.position = "right",
legend.key.size = unit(0.6, "lines"))
plot_positions
df_to_classify = sunflower::IGC_long %>% select(-c(modality, task_modality,task_type, test, task))
m_w2v = word2vec::read.word2vec(file = "dependency-bundle/sbw_vectors.bin", normalize = TRUE)
errors_classified = df_to_classify %>%
check_lexicality(item_col = "item", response_col = "Response", criterion = "database") %>%
get_formal_similarity(item_col = "item", response_col = "Response",
attempt_col = "Attempt", group_cols = c("ID", "item_ID")) %>%
get_semantic_similarity(item_col = "item", response_col = "Response", model = m_w2v) %>%
classify_errors(response_col = "Response", item_col = "item",
access_col = "accessed", RA_col = "RA", also_classify_RAs = T)
errors_classified = df_to_classify %>%
check_lexicality(item_col = "item", response_col = "Response", criterion = "database") %>%
get_formal_similarity(item_col = "item", response_col = "Response",
attempt_col = "Attempt", group_cols = c("ID", "item_ID")) %>%
get_semantic_similarity(item_col = "item", response_col = "Response", model = m_w2v) %>%
classify_errors(response_col = "Response", item_col = "item",
access_col = "accessed", RA_col = "RA", also_classify_RAs = T)
document()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
require("sunflower")
require("tidyverse")
require("htmltools")
df_to_formal_metrics = sunflower::IGC_long_phon %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item",
response_col = "response",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
formal_metrics_computed %>% head(8) %>% knitr::kable()
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(item_col = "item", response_col = "response", match_col = "itemL_adj_strict_match_pos")
positions_accuracy %>% head(8) %>% knitr::kable()
# Convertir targetL a character para evitar problemas al combinar dataframes
positions_accuracy <- positions_accuracy %>%
mutate(targetL = as.character(targetL))
# Duplicar y modificar el dataframe para crear 'positions_general'
positions_general <- positions_accuracy %>%
mutate(targetL = "General")
# Combinar ambos dataframes
positions <- bind_rows(positions_accuracy, positions_general)
# Especificar manualmente los niveles en el orden deseado
desired_levels <- c("3", "4", "5", "6", "7", "8", "9", "10", "11", "12",
"13", "14", "15", "17", "21", "22", "24", "48", "General")
# Convertir correct_pos a numérico y ordenar targetL como factor según desired_levels
positions <- positions %>%
mutate(correct_pos = as.numeric(correct_pos),
targetL = factor(targetL, levels = desired_levels)) %>%
arrange(correct_pos, targetL)
# Definir un conjunto de linetypes que se pueda repetir
custom_linetypes <- rep(c("solid", "dashed", "dotted", "longdash", "dotdash"),
length.out = nlevels(positions$targetL))
# Calcular la precisión y contar el número de observaciones por grupo
plot_positions <- positions %>%
group_by(Position, targetL) %>%
summarize(acc = mean(correct_pos, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x = as.numeric(Position), y = acc, group = targetL,
fill = targetL, color = targetL, lty = targetL)) +
geom_line(size = 0.70, alpha = 0.6) +
geom_point(aes(size = n), shape = 21, color = "black", alpha = 0.6) +
scale_linetype_manual(values = custom_linetypes) +
theme(panel.border = element_rect(colour = "black", fill = NA),
panel.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line = element_blank()) +
ylab("Proportion (%) of correct phonemes") +
xlab("Phoneme position") +
guides(fill = guide_legend(title = "Word Length"),
lty = guide_legend(title = "Word Length"),
color = guide_legend(title = "Word Length"),
size = guide_legend(title = "Datapoints")) +
theme_gray() +
theme(legend.position = "right",
legend.key.size = unit(0.6, "lines"))
plot_positions
df_to_classify = sunflower::IGC_long %>% select(-c(modality, task_modality,task_type, test, task))
m_w2v = word2vec::read.word2vec(file = "dependency-bundle/sbw_vectors.bin", normalize = TRUE)
errors_classified = df_to_classify %>%
check_lexicality(item_col = "item", response_col = "Response", criterion = "database") %>%
get_formal_similarity(item_col = "item", response_col = "Response",
attempt_col = "Attempt", group_cols = c("ID", "item_ID")) %>%
get_semantic_similarity(item_col = "item", response_col = "Response", model = m_w2v) %>%
classify_errors(response_col = "Response", item_col = "item",
access_col = "accessed", RA_col = "RA", also_classify_RAs = T)
document()
print()
print(2)
install()
print(3)
errors_classified = df_to_classify %>%
document()
install()
df_to_formal_metrics = sunflower::load("data/mine/IGC_long.RData") %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
df_to_formal_metrics = load("data/mine/IGC_long.RData") %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
df_to_formal_metrics = sunflower::IGC_long %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item",
response_col = "response",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item",
response_col = "Response",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
df_to_formal_metrics = sunflower::IGC_long_phon %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item",
response_col = "response",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
formal_metrics_computed %>% head(8) %>% knitr::kable()
***Note.*** A plot depicting the positions' accuracy of nrow(positions) datapoints.
nrow(positions)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
require("sunflower")
require("tidyverse")
require("htmltools")
df_to_formal_metrics = sunflower::IGC_long_phon %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item",
response_col = "response",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
formal_metrics_computed %>% head(8) %>% knitr::kable()
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(item_col = "item", response_col = "response", match_col = "itemL_adj_strict_match_pos")
positions_accuracy %>% head(8) %>% knitr::kable()
# Convertir targetL a character para evitar problemas al combinar dataframes
positions_accuracy <- positions_accuracy %>%
mutate(targetL = as.character(targetL))
# Duplicar y modificar el dataframe para crear 'positions_general'
positions_general <- positions_accuracy %>%
mutate(targetL = "General")
# Combinar ambos dataframes
positions <- bind_rows(positions_accuracy, positions_general)
# Especificar manualmente los niveles en el orden deseado
desired_levels <- c("3", "4", "5", "6", "7", "8", "9", "10", "11", "12",
"13", "14", "15", "17", "21", "22", "24", "48", "General")
# Convertir correct_pos a numérico y ordenar targetL como factor según desired_levels
positions <- positions %>%
mutate(correct_pos = as.numeric(correct_pos),
targetL = factor(targetL, levels = desired_levels)) %>%
arrange(correct_pos, targetL)
# Definir un conjunto de linetypes que se pueda repetir
custom_linetypes <- rep(c("solid", "dashed", "dotted", "longdash", "dotdash"),
length.out = nlevels(positions$targetL))
# Calcular la precisión y contar el número de observaciones por grupo
plot_positions <- positions %>%
group_by(Position, targetL) %>%
summarize(acc = mean(correct_pos, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x = as.numeric(Position), y = acc, group = targetL,
fill = targetL, color = targetL, lty = targetL)) +
geom_line(size = 0.70, alpha = 0.6) +
geom_point(aes(size = n), shape = 21, color = "black", alpha = 0.6) +
scale_linetype_manual(values = custom_linetypes) +
theme(panel.border = element_rect(colour = "black", fill = NA),
panel.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line = element_blank()) +
ylab("Proportion (%) of correct phonemes") +
xlab("Phoneme position") +
guides(fill = guide_legend(title = "Word Length"),
lty = guide_legend(title = "Word Length"),
color = guide_legend(title = "Word Length"),
size = guide_legend(title = "Datapoints")) +
theme_gray() +
theme(legend.position = "right",
legend.key.size = unit(0.6, "lines"))
nrow(positions)
positions_accuracy %>% head(8) %>% knitr::kable() %>% select(ID:response_phon)
positions_accuracy %>% select(ID:response_phon) %>% head(8) %>% knitr::kable()
positions_accuracy
positions_accuracy %>% select(ID:response_phon, RA, Attempt, Position:element_in_response) %>% head(8) %>% knitr::kable()
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(item_col = "item_phon", response_col = "response_phon",
match_col = "itemL_adj_strict_match_pos")
positions_accuracy %>% select(ID:response_phon, RA, Attempt, Position:element_in_response) %>% head(8) %>% knitr::kable()
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item_phon",
response_col = "response_phon",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
formal_metrics_computed %>% head(8) %>% knitr::kable()
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(item_col = "item_phon", response_col = "response_phon",
match_col = "itemL_adj_strict_match_pos")
positions_accuracy %>% select(ID:response_phon, RA, Attempt, Position:element_in_response) %>% head(8) %>% knitr::kable()
# Convertir targetL a character para evitar problemas al combinar dataframes
positions_accuracy <- positions_accuracy %>%
mutate(targetL = as.character(targetL))
# Duplicar y modificar el dataframe para crear 'positions_general'
positions_general <- positions_accuracy %>%
mutate(targetL = "General")
# Combinar ambos dataframes
positions <- bind_rows(positions_accuracy, positions_general)
# Especificar manualmente los niveles en el orden deseado
desired_levels <- c("3", "4", "5", "6", "7", "8", "9", "10", "11", "12",
"13", "14", "15", "17", "21", "22", "24", "48", "General")
# Convertir correct_pos a numérico y ordenar targetL como factor según desired_levels
positions <- positions %>%
mutate(correct_pos = as.numeric(correct_pos),
targetL = factor(targetL, levels = desired_levels)) %>%
arrange(correct_pos, targetL)
# Definir un conjunto de linetypes que se pueda repetir
custom_linetypes <- rep(c("solid", "dashed", "dotted", "longdash", "dotdash"),
length.out = nlevels(positions$targetL))
# Calcular la precisión y contar el número de observaciones por grupo
plot_positions <- positions %>%
group_by(Position, targetL) %>%
summarize(acc = mean(correct_pos, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x = as.numeric(Position), y = acc, group = targetL,
fill = targetL, color = targetL, lty = targetL)) +
geom_line(size = 0.70, alpha = 0.6) +
geom_point(aes(size = n), shape = 21, color = "black", alpha = 0.6) +
scale_linetype_manual(values = custom_linetypes) +
theme(panel.border = element_rect(colour = "black", fill = NA),
panel.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line = element_blank()) +
ylab("Proportion (%) of correct phonemes") +
xlab("Phoneme position") +
guides(fill = guide_legend(title = "Word Length"),
lty = guide_legend(title = "Word Length"),
color = guide_legend(title = "Word Length"),
size = guide_legend(title = "Datapoints")) +
theme_gray() +
theme(legend.position = "right",
legend.key.size = unit(0.6, "lines"))
df_to_classify = sunflower::IGC_long %>% select(-c(modality, task_modality,task_type, test, task))
m_w2v = word2vec::read.word2vec(file = "dependency-bundle/sbw_vectors.bin", normalize = TRUE)
errors_classified = df_to_classify %>%
check_lexicality(item_col = "item", response_col = "Response", criterion = "database") %>%
get_formal_similarity(item_col = "item", response_col = "Response",
attempt_col = "Attempt", group_cols = c("ID", "item_ID")) %>%
get_semantic_similarity(item_col = "item", response_col = "Response", model = m_w2v) %>%
classify_errors(response_col = "Response", item_col = "item",
access_col = "accessed", RA_col = "RA", also_classify_RAs = T)
errors_classified %>% head(8) %>% knitr::kable()
df_to_formal_metrics = load("data/mine/IGC_long_phon.Rdata") %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
df_to_formal_metrics = load("data/mine/IGC_long_phon_mine.Rdata") %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
df_to_formal_metrics = load("data/mine/IGC_long_phon_mine") %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
df_to_formal_metrics = load("data/mine/IGC_long_phon_mine.Rda") %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
load("data/mine/IGC_long_phon_mine.Rda")
a = load("data/mine/IGC_long_phon_mine.Rda")
load("data/mine/IGC_long_phon_mine.Rda")
load("data/mine/IGC_long_phon_mine.Rda")
errors_classified = errors_classified %>%
dplyr::filter(general_ID %in% c(8, 156, 13, 3284, 222, 3448, 5658)) %>% ####AQUI SELECCIONAR UN EJEMPLO DE CADA
dplyr::select(general_ID, ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)
errors_classified = errors_classified %>%
dplyr::filter(general_ID %in% c(8, 156, 13, 3284, 222, 3448, 5658)) %>%
dplyr::select(general_ID, ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
require("sunflower")
require("tidyverse")
require("htmltools")
load("data/mine/IGC_long_phon_mine.Rda")
df_to_formal_metrics = IGC_long_phon %>%
dplyr::select(-c(modality, task_modality,task_type, test, task)) # keep some relevant columns
formal_metrics_computed = df_to_formal_metrics %>%
get_formal_similarity(item_col = "item_phon",
response_col = "response_phon",
attempt_col = "Attempt",
group_cols = c("ID", "item_ID"))
formal_metrics_computed %>% head(8) %>% knitr::kable()
positions_accuracy = formal_metrics_computed %>%
positional_accuracy(item_col = "item_phon", response_col = "response_phon",
match_col = "itemL_adj_strict_match_pos")
positions_accuracy %>% select(ID:response_phon, RA, Attempt, Position:element_in_response) %>% head(8) %>% knitr::kable()
# Convertir targetL a character para evitar problemas al combinar dataframes
positions_accuracy <- positions_accuracy %>%
mutate(targetL = as.character(targetL))
# Duplicar y modificar el dataframe para crear 'positions_general'
positions_general <- positions_accuracy %>%
mutate(targetL = "General")
# Combinar ambos dataframes
positions <- bind_rows(positions_accuracy, positions_general)
# Especificar manualmente los niveles en el orden deseado
desired_levels <- c("3", "4", "5", "6", "7", "8", "9", "10", "11", "12",
"13", "14", "15", "17", "21", "22", "24", "48", "General")
# Convertir correct_pos a numérico y ordenar targetL como factor según desired_levels
positions <- positions %>%
mutate(correct_pos = as.numeric(correct_pos),
targetL = factor(targetL, levels = desired_levels)) %>%
arrange(correct_pos, targetL)
# Definir un conjunto de linetypes que se pueda repetir
custom_linetypes <- rep(c("solid", "dashed", "dotted", "longdash", "dotdash"),
length.out = nlevels(positions$targetL))
# Calcular la precisión y contar el número de observaciones por grupo
plot_positions <- positions %>%
group_by(Position, targetL) %>%
summarize(acc = mean(correct_pos, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x = as.numeric(Position), y = acc, group = targetL,
fill = targetL, color = targetL, lty = targetL)) +
geom_line(size = 0.70, alpha = 0.6) +
geom_point(aes(size = n), shape = 21, color = "black", alpha = 0.6) +
scale_linetype_manual(values = custom_linetypes) +
theme(panel.border = element_rect(colour = "black", fill = NA),
panel.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.line = element_blank()) +
ylab("Proportion (%) of correct phonemes") +
xlab("Phoneme position") +
guides(fill = guide_legend(title = "Word Length"),
lty = guide_legend(title = "Word Length"),
color = guide_legend(title = "Word Length"),
size = guide_legend(title = "Datapoints")) +
theme_gray() +
theme(legend.position = "right",
legend.key.size = unit(0.6, "lines"))
plot_positions
df_to_classify = sunflower::IGC_long %>% select(-c(modality, task_modality,task_type, test, task))
m_w2v = word2vec::read.word2vec(file = "dependency-bundle/sbw_vectors.bin", normalize = TRUE)
errors_classified = df_to_classify %>%
check_lexicality(item_col = "item", response_col = "Response", criterion = "database") %>%
get_formal_similarity(item_col = "item", response_col = "Response",
attempt_col = "Attempt", group_cols = c("ID", "item_ID")) %>%
get_semantic_similarity(item_col = "item", response_col = "Response", model = m_w2v) %>%
classify_errors(response_col = "Response", item_col = "item",
access_col = "accessed", RA_col = "RA", also_classify_RAs = T)
errors_classified = errors_classified %>%
dplyr::filter(general_ID %in% c(8, 156, 13, 3284, 222, 3448, 5658)) %>%
dplyr::select(general_ID, ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)
errors_classified
errors_classified = errors_classified %>%
dplyr::filter(ID %in% c(8, 156, 13, 3284, 222, 3448, 5658)) %>%
dplyr::select(ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)
errors_classified %>%
select(ID, item_ID, item, Response, RA, Attempt, nonword:comment) %>%
head(8) %>% knitr::kable()
errors_classified
errors_classified = df_to_classify %>%
check_lexicality(item_col = "item", response_col = "Response", criterion = "database") %>%
get_formal_similarity(item_col = "item", response_col = "Response",
attempt_col = "Attempt", group_cols = c("ID", "item_ID")) %>%
get_semantic_similarity(item_col = "item", response_col = "Response", model = m_w2v) %>%
classify_errors(response_col = "Response", item_col = "item",
access_col = "accessed", RA_col = "RA", also_classify_RAs = T)
errors_classified = errors_classified %>%
dplyr::select(ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)
errors_classified
errors_classified = errors_classified %>%
dplyr::select(ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment) %>%
dplyr::filter(ID %in% c(8, 156, 13, 3284, 222, 3448, 5658))
errors_classified = df_to_classify %>%
check_lexicality(item_col = "item", response_col = "Response", criterion = "database") %>%
get_formal_similarity(item_col = "item", response_col = "Response",
attempt_col = "Attempt", group_cols = c("ID", "item_ID")) %>%
get_semantic_similarity(item_col = "item", response_col = "Response", model = m_w2v) %>%
classify_errors(response_col = "Response", item_col = "item",
access_col = "accessed", RA_col = "RA", also_classify_RAs = T)
errors_classified = errors_classified %>%
dplyr::select(ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)
errors_classified %>% dplyr::filter(ID %in% c(8, 156, 13, 3284, 222, 3448, 5658))
View(errors_classified)
errors_classified = errors_classified %>%
dplyr::select(ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)%>%
dplyr::mutate(general_ID = dplyr::row_number())
errors_classified = errors_classified %>%
dplyr::select(ID, item_ID, item, Response, RA,
Attempt, lexicality, cosine_similarity, nonword:no_response, comment)%>%
dplyr::mutate(general_ID = dplyr::row_number())
errors_classified %>% dplyr::filter(general_ID %in% c(8, 156, 13, 3284, 222, 3448, 5658))
errors_classified %>% dplyr::filter(general_ID %in% c(8, 156, 13, 3284, 222, 3448, 5658)) %>% View()
View(errors_classified)
# Crear o editar el archivo .gitignore
gitignore_path <- ".gitignore"
ignore_content <- c(
"dependency-bundle/",
"R/old_functions/",
"data/mine/",
"raw-data/",
"manuscript/"
)
writeLines(ignore_content, gitignore_path)
# Eliminar directorios del índice de Git
dirs_to_remove <- c("dependency-bundle", "R/old_functions", "data/mine", "raw-data", "manuscript")
# Bucle para eliminar los directorios del índice de Git
for (dir in dirs_to_remove) {
system(paste("git rm -r --cached", dir))
}
# Agregar los cambios al índice
system("git add .")
# Hacer commit
system("git commit -m 'Aplicar .gitignore y eliminar directorios rastreados'")
# Verificar que los directorios están siendo ignorados
for (dir in dirs_to_remove) {
check_ignore_command <- paste("git check-ignore -v", dir)
system(check_ignore_command)
}
git rm -r --cached dependency-bundle/
